# savedsearches.conf

# Data sampling - detection of raw event format changes
[TrackMe - Data sampling and format detection tracker]
cron_schedule = */15 * * * *
description = This scheduled report tracks for data sources format change detection, aka data sampling
dispatch.earliest_time = -24h
dispatch.latest_time = +4h
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 300 # 5m ttl for this artefact
search = | savedsearch runSPL [\
\
| inputlookup trackme_data_source_monitoring where data_monitored_state="enabled" | eval key=_key | where data_last_time_seen>relative_time(now(), "-24h") | sort limit=0 data_sample_lastrun | head [ `trackme_data_sampling_algo_entities_to_process` ]\
\
`comment("##### Once the KVstore content is loaded, we need to adress the specific case of Elastic sources #####")`\
| `trackme_lookup_elastic_sources_for_data_sampling`\
\
`comment("##### Define the search contraint dynamically #####")`\
| eval search_constraint = if(isnull(elastic_source_search_constraint), "index=\"" . data_index . "\" " . "sourcetype=\"" . data_sourcetype . "\"", elastic_source_search_constraint)\
| fields key, data_name, data_sourcetype, search_constraint, elastic_source_search_constraint, elastic_source_search_mode | where NOT (elastic_source_search_mode="mstats" OR elastic_source_search_mode="from")\
\
`comment("##### Lookup the data sampling collection to retrieve the iteration ID of the data sampling, we use this to conditionally define the number of events to be retrieved at the first iteration #####")`\
`comment("##### In addition filter on entries where the feature is enabled or new entries only")`\
| lookup trackme_data_sampling _key as key OUTPUT data_sample_iteration, data_sample_feature, data_sampling_nr, data_sample_anomaly_ack_status\
| fillnull value="enabled" data_sample_feature | where NOT (data_sample_feature == "disabled")\
| eval data_sample_iteration=if(isnull(data_sample_iteration), 0, data_sample_iteration)\
`comment("##### Records in current anomaly state are not to be handled")`\
| fillnull value="N/A" data_sample_anomaly_ack_status\
| where data_sample_anomaly_ack_status!="uncleared"\
\
`comment("##### Finally generate the search to be used, the number of events sample to retrieve depends if this is the iteration for this entity #####")`\
\
`comment("##### If a number of records to sample is configured, this value will be used in priority, otherwise we define these via macro and conditions #####")`\
| eval events_sample_range=if(isnum(data_sampling_nr), data_sampling_nr, if(data_sample_iteration=0, `trackme_data_sampling_default_sample_record_at_discovery`, `trackme_data_sampling_default_sample_record_at_run`) )\
\
| eval spl=search_constraint . " | head " . events_sample_range . " | eval key = \"" . key . "\" | eval data_name = \"" . data_name . "\" | stats values(_raw) as raw_sample by key, data_name | eval data_sourcetype = \"" . data_sourcetype . "\" | mvexpand raw_sample"\
| fields spl\
\
| eval prefix="| append [ search "\
| eval suffix=" ]"\
\
| streamstats count as line_count\
| eval spl = if(line_count!=1, prefix . spl . suffix, spl)\
\
| fields spl\
\
| mvcombine delim=" " spl | nomv spl\
\
| append [ | makeresults | eval spl=if(isnull(spl), "| makeresults", spl) | fields - _time ] | head 1\
\
]\
\
`comment("##### To avoid reaching KVstore document max size, limit the content of raw events to be stored #####")`\
| eval raw_sample=substr(raw_sample, 1, 50000)\
\
`comment("##### Call the data sampling engine macro #####")`\
| `trackme_data_sampling_engine`\
\
`comment("##### Update the KVstore content #####")`\
| outputlookup append=t key_field=key trackme_data_sampling\
| stats c by data_name

# Data sampling - detection of raw event format changes
[TrackMe - Data sampling engine for target]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | savedsearch runSPL [\
\
| inputlookup trackme_data_source_monitoring where _key="$key$" | eval key=_key\
`comment("##### Once the KVstore content is loaded, we need to adress the specific case of Elastic sources #####")`\
| `trackme_lookup_elastic_sources`\
\
`comment("##### Define the search contraint dynamically #####")`\
| eval search_constraint = if(isnull(elastic_source_search_constraint), "index=\"" . data_index . "\" " . "sourcetype=\"" . data_sourcetype . "\"", elastic_source_search_constraint)\
| fields key, data_name, data_sourcetype, search_constraint, elastic_source_search_constraint, elastic_source_search_mode | where NOT (elastic_source_search_mode="mstats" OR elastic_source_search_mode="from")\
\
`comment("##### Lookup the data sampling collection to retrieve the iteration ID of the data sampling, we use this to conditionally define the number of events to be retrieved at the first iteration #####")`\
`comment("##### In addition filter on entries where the feature is enabled or new entries only")`\
| lookup trackme_data_sampling _key as key OUTPUT data_sample_iteration, data_sample_feature, data_sampling_nr, data_sample_anomaly_ack_status\
| fillnull value="enabled" data_sample_feature | where NOT (data_sample_feature == "disabled")\
| eval data_sample_iteration=if(isnull(data_sample_iteration), 0, data_sample_iteration)\
`comment("##### Records in current anomaly state are not to be handled")`\
| fillnull value="N/A" data_sample_anomaly_ack_status\
| where data_sample_anomaly_ack_status!="uncleared"\
\
`comment("##### Finally generate the search to be used, the number of events sample to retrieve depends if this is the iteration for this entity #####")`\
\
`comment("##### If a number of records to sample is configured, this value will be used in priority, otherwise we define these via macro and conditions #####")`\
| eval events_sample_range=if(isnum(data_sampling_nr), data_sampling_nr, if(data_sample_iteration=0, `trackme_data_sampling_default_sample_record_at_discovery`, `trackme_data_sampling_default_sample_record_at_run`) )\
\
| eval spl=search_constraint . " | head " . events_sample_range . " | eval key = \"" . key . "\" | eval data_name = \"" . data_name . "\" | stats values(_raw) as raw_sample by key, data_name | eval data_sourcetype = \"" . data_sourcetype . "\" | mvexpand raw_sample"\
| fields spl\
\
| eval prefix="| append [ search "\
| eval suffix=" ]"\
\
| streamstats count as line_count\
| eval spl = if(line_count!=1, prefix . spl . suffix, spl)\
\
| fields spl\
\
| mvcombine delim=" " spl | nomv spl\
\
| append [ | makeresults | eval spl=if(isnull(spl), "| makeresults", spl) | fields - _time ] | head 1\
\
]\
\
`comment("##### Call the data sampling engine macro #####")`\
| `trackme_data_sampling_engine`\
\
`comment("##### Update the KVstore content #####")`\
| outputlookup append=t key_field=key trackme_data_sampling

# Monitoring of elastic data sources
[TrackMe - Elastic sources shared tracker]
cron_schedule = */5 * * * *
description = This scheduled report tracks and updates the data source availability KVstore based lookup
dispatch.earliest_time = -4h
dispatch.latest_time = +4h
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 300 # 5m ttl for this artefact
search= | savedsearch runSPL [\
\
`comment("#### Load the KVstore collection ####")`\
| inputlookup trackme_elastic_sources\
\
`comment("#### specific for from type of searches ####")`\
| rex field=search_constraint "^(?<from_part1>[^\|]*)\|{0,1}(?<from_part2>.*)?" | eval from_part2=if(search_mode="from" AND (isnull(from_part2) OR from_part2=""), "search *", from_part2)\
| rex field=search_constraint "(?<rest_target>(?:splunk_server|splunk_server_group)\=[^\|]*)\s{0,}\|\s{0,}(?<rest_constrainst>.*)"\
| rex field=search_constraint "(?<rest_target>(?:splunk_server|splunk_server_group)\=[^\|]*)\s{0,}\|\s{0,}(?<rest_constrainst_part1>[^\|]*\s{0,})\|{0,1}\s{0,}(?<rest_constrainst_part2>.*)"\
| rex field=rest_constrainst mode=sed "s/\"/\\\"/g"\
| rex field=rest_constrainst_part1 mode=sed "s/\"/\\\"/g"\
| rex field=rest_constrainst_part2 mode=sed "s/\"/\\\"/g"\
| eval rest_constrainst_part2=if(search_mode="rest_from" AND (isnull(rest_constrainst_part2) OR rest_constrainst_part2=""), "search *", rest_constrainst_part2)\
\
`comment("#### conditionally generates the SPL code ####")`\
\
| eval spl=case(\
search_mode="tstats", "| " . search_mode . " max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " where " . search_constraint . " | eval data_name=\"" . data_name . "\", data_index=\"" . elastic_data_index . "\", data_sourcetype=\"" . elastic_data_sourcetype . "\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen",\
search_mode="raw", "search " . search_constraint . " | stats max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " | eval data_name=\"" . data_name . "\", data_index=\"" . elastic_data_index . "\", data_sourcetype=\"" . elastic_data_sourcetype . "\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen",\
search_mode="from" AND match(from_part1, "(?i)datamodel\:"), "| " . search_mode . " " . from_part1 . " | " . from_part2 . " | stats max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " | eval data_name=\"" . data_name . "\", data_index=\"" . elastic_data_index . "\", data_sourcetype=\"" . elastic_data_sourcetype . "\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen",\
search_mode="from" AND match(from_part1, "(?i)lookup\:"), "| " . search_mode . " " . from_part1 . " | " . from_part2 . " | eventstats max(_time) as indextime | eval _indextime=if(isnum(_indextime), _indextime, indextime) | fields - indextime | eval host=if(isnull(host), \"none\", host) | stats max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " | eval data_name=\"" . data_name . "\", data_index=\"" . elastic_data_index . "\", data_sourcetype=\"" . elastic_data_sourcetype . "\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen",\
search_mode="mstats", "| " . search_mode . " latest(_value) as value" . " where " . search_constraint . " by host, metric_name span=1s | stats min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, dc(metric_name) as data_eventcount, dc(host) as dcount_host | eval data_name=\"" . data_name . "\", data_index=\"" . elastic_data_index . "\", data_sourcetype=\"" . elastic_data_sourcetype . "\", data_last_ingest=data_last_time_seen, data_last_ingestion_lag_seen=now()-data_last_time_seen",\
search_mode="rest_tstats", "| rest " . rest_target . " /servicesNS/admin/search/search/jobs/export search=\"" . "| tstats" . " max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " where " . rest_constrainst . " | eval data_name=\\\"" . data_name . "\\\", data_index=\\\"" . elastic_data_index . "\\\", data_sourcetype=\\\"" . elastic_data_sourcetype . "\\\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\"",\
search_mode="rest_raw", "| rest " . rest_target . " /servicesNS/admin/search/search/jobs/export search=\"" . "search " . rest_constrainst . " | stats max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " | eval data_name=\\\"" . data_name . "\\\", data_index=\\\"" . elastic_data_index . "\\\", data_sourcetype=\\\"" . elastic_data_sourcetype . "\\\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\"",\
search_mode="rest_from" AND match(rest_constrainst_part1, "(?i)datamodel\:"), "| rest " . rest_target . " /servicesNS/admin/search/search/jobs/export search=\"" . "| from " . rest_constrainst_part1 . " | " . rest_constrainst_part2 . " | stats max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " | eval data_name=\\\"" . data_name . "\\\", data_index=\\\"" . elastic_data_index . "\\\", data_sourcetype=\\\"" . elastic_data_sourcetype . "\\\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\"",\
search_mode="rest_from" AND match(rest_constrainst_part1, "(?i)lookup\:"), "| rest " . rest_target . " /servicesNS/admin/search/search/jobs/export search=\"" . "| from " . rest_constrainst_part1 . " | " . rest_constrainst_part2 . " | eventstats max(_time) as indextime | eval _indextime=if(isnum(_indextime), _indextime, indextime) | fields - indextime | eval host=if(isnull(host), \\\"none\\\", host) | stats max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount, dc(host) as dcount_host" . " | eval data_name=\\\"" . data_name . "\\\", data_index=\\\"" . elastic_data_index . "\\\", data_sourcetype=\\\"" . elastic_data_sourcetype . "\\\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\"",\
search_mode="rest_mstats", "| rest " . rest_target . " /servicesNS/admin/search/search/jobs/export search=\"" . "| mstats" . " latest(_value) as value" . " where " . rest_constrainst . " by host, metric_name span=1s | stats min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, dc(metric_name) as data_eventcount, dc(host) as dcount_host | eval data_last_ingest=data_last_time_seen | eval data_name=\\\"" . data_name . "\\\", data_index=\\\"" . elastic_data_index . "\\\", data_sourcetype=\\\"" . elastic_data_sourcetype . "\\\", data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\""\
)\
| eval spl=if(match(search_mode, "^rest_\w+"), spl . " output_mode=\"csv\" " . " earliest_time=\"-4h\" " . " latest_time=\"+4h\" " . "| head 1 | table value | restextract", spl)\
| fields spl\
\
| eval prefix="| append [ "\
| eval suffix=" ]"\
\
| streamstats count as line_count\
| eval spl = if(line_count!=1, prefix . spl . suffix, spl)\
\
| fields spl\
\
| mvcombine delim=" " spl | nomv spl\
\
| append [ | makeresults | eval spl=if(isnull(spl), "| makeresults", spl) | fields - _time ] | rex field=spl mode=sed "s/^search\s//" | head 1\
\
]\
| where isnotnull(data_name) AND data_eventcount>0\
`comment("#### The macro expects a different name for the first time seen ####")`\
| rename data_first_time_seen as data_first_time_seen\
\
`comment("#### Specific to elastic sources ####")`\
| eval data_index=if(isnull(data_index) OR data_index="none", data_name, data_index)\
| eval data_sourcetype=if(isnull(data_sourcetype) OR data_sourcetype="none", data_name, data_sourcetype)\
\
`comment("#### call the abstract macro ####")`\
`trackme_data_source_tracker_abstract`\
\
`comment("#### Restrict to shared trackers only ####")`\
| search [ | inputlookup append=t trackme_elastic_sources | fields data_name | format | fields search ]\
\
`comment("#### collects latest collection state into the summary index ####")`\
| `trackme_collect_state("current_state_tracking:data_source", "data_name")`\
\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(data_source_state, data_previous_source_state, data_name, trackme_audit_flip_temp_data_source_shared)`\
\
`comment("#### run collect and updates the KVstore ####")`\
| `trackme_outputlookup(trackme_data_source_monitoring, key)`\
| `trackme_mcollect(data_name, data_source, "metric_name:trackme.eventcount_4h=data_eventcount, metric_name:trackme.hostcount_4h=dcount_host, metric_name:trackme.lag_event_sec=data_last_lag_seen, metric_name:trackme.lag_ingestion_sec=data_last_ingestion_lag_seen", "object_category, object, OutlierTimePeriod, enable_behaviour_analytic")`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_data_source_shared)`\
| stats c

# Monitoring of data sources

[TrackMe - Data sources abstract root tracker]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | `trackme_tstats` max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount where index=* sourcetype=* `trackme_tstats_main_filter` `trackme_get_idx_whitelist(trackme_data_source_monitoring_whitelist_index, data_index)` `apply_data_source_blacklists_data_retrieve` by index, sourcetype, host\
\
`comment("#### tstats result table is loaded ####")`\
| eval data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\
\
`comment("#### intermediate calculation ####")`\
| `trackme_default_data_source_mode`\
\
`comment("#### call the abstract macro ####")`\
`trackme_data_source_tracker_abstract`

[TrackMe - Data sources availability short term tracker]
cron_schedule = */5 * * * *
description = This scheduled report tracks and updates the data source availability KVstore based lookup
dispatch.earliest_time = -4h
dispatch.latest_time = +4h
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 300 # 5m ttl for this artefact
search  = | savedsearch "TrackMe - Data sources abstract root tracker"\
\
`comment("#### Exclude Elastic sources which are managed by the Elastic shared tracker or dedicated Elastic trackers ####")`\
| search NOT [ | inputlookup append=t trackme_elastic_sources | inputlookup append=t trackme_elastic_sources_dedicated | fields data_name | format | fields search ]\
\
`comment("#### collects latest collection state into the summary index ####")`\
| `trackme_collect_state("current_state_tracking:data_source", "data_name")`\
\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(data_source_state, data_previous_source_state, data_name, trackme_audit_flip_temp_data_source)`\
| `trackme_outputlookup(trackme_data_source_monitoring, key)`\
| where data_source_is_online="true"\
| `trackme_mcollect(data_name, data_source, "metric_name:trackme.eventcount_4h=data_eventcount, metric_name:trackme.hostcount_4h=dcount_host, metric_name:trackme.lag_event_sec=data_last_lag_seen, metric_name:trackme.lag_ingestion_sec=data_last_ingestion_lag_seen", "object_category, object, OutlierTimePeriod, enable_behaviour_analytic")`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_data_source)`\
| stats c

[TrackMe - Data sources availability long term tracker]
cron_schedule = 1 * * * *
description = This scheduled report tracks and updates the data source availability KVstore based lookup
dispatch.earliest_time = -7d
dispatch.latest_time = +4h
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 900 # 15m ttl for this artefact
search  = | savedsearch "TrackMe - Data sources abstract root tracker"\
\
`comment("#### Exclude Elastic sources which are managed by the Elastic shared tracker or dedicated Elastic trackers ####")`\
| search NOT [ | inputlookup append=t trackme_elastic_sources | inputlookup append=t trackme_elastic_sources_dedicated | fields data_name | format | fields search ]\
\
| where data_last_time_seen<relative_time(now(), "-4h-5m")\
\
`comment("#### collects latest collection state into the summary index ####")`\
| `trackme_collect_state("current_state_tracking:data_source", "data_name")`\
\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(data_source_state, data_previous_source_state, data_name, trackme_audit_flip_temp_data_source)`\
| `trackme_outputlookup(trackme_data_source_monitoring, key)`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_data_source)`\
| stats c

[TrackMe - Alert on data source availability]
alert.digest_mode = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.fields = data_name, data_index, data_sourcetype
alert.suppress.period = 24h
alert.track = 1
counttype = number of events
cron_schedule = */5 * * * *
description = This alert will trigger if one or more of the data hosts are detected as unavailable or not honouring SLA policies.
dispatch.earliest_time = -5m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
disabled = true
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = trackme
request.ui_dispatch_view = search
search = | inputlookup trackme_data_source_monitoring\
| appendcols [ | inputlookup trackme_maintenance_mode ] | filldown maintenance_mode | where NOT maintenance_mode="enabled"\
| where data_monitored_state="enabled" AND `trackme_alerts_priority`\
| `trackme_eval_data_source_state`\
| search `trackme_get_idx_whitelist_searchtime(trackme_data_source_monitoring_whitelist_index, data_index)`\
| `apply_data_source_blacklists`\
| where data_source_state="red"\
| `trackme_date_format(data_last_ingest)`\
| `trackme_date_format(data_last_time_seen)`\
| `trackme_date_format(data_tracker_runtime)`\
| `trackme_date_format(data_previous_tracker_runtime)`\
| `trackme_eval_icons`\
| rename "* (translated)" as "*"\
| `trackme_get_identity_card(data_name)`\
| `trackme_ack_lookup(data_name, data_source)`\
| fields - monitoring, state\
| `trackme_alerts_order_data_source`

# Monitoring of hosts

#
# Host monitoring
#

[TrackMe - Data hosts abstract root tracker]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | `trackme_tstats` max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount where index=* sourcetype=* host=* host!="" `trackme_tstats_main_filter` `trackme_get_idx_whitelist(trackme_data_host_monitoring_whitelist_index, data_index)` `apply_data_host_blacklists_data_retrieve` by index, sourcetype, host\
`comment("#### tstats result table is loaded ####")`\
\
`comment("#### define a value for the last seen ingestion lag for that source ####")`\
| eval data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\
\
`comment("#### Cal the asbtract macro ####")`\
| `trackme_data_host_tracker_abstract`

[TrackMe - hosts availability short term tracker]
cron_schedule = */5 * * * *
description = This scheduled report tracks and updates the data source availability KVstore based lookup
dispatch.earliest_time = -4h
dispatch.latest_time = +4h
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 300 # 5m ttl for this artefact
search  = | savedsearch "TrackMe - Data hosts abstract root tracker"\
\
`comment("#### collects latest collection state into the summary index ####")`\
| `trackme_collect_state("current_state_tracking:data_host", "data_host")`\
\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(data_host_state, data_previous_host_state, data_host, trackme_audit_flip_temp_data_host)`\
| `trackme_outputlookup(trackme_host_monitoring, key)`\
| where data_host_is_online="true"\
| `trackme_mcollect(data_host, data_host, "metric_name:trackme.eventcount_4h=data_eventcount, metric_name:trackme.hostcount_4h=dcount_host, metric_name:trackme.lag_event_sec=data_last_lag_seen, metric_name:trackme.lag_ingestion_sec=data_last_ingestion_lag_seen", "object_category, object, OutlierTimePeriod, enable_behaviour_analytic")`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_data_host)`\
| stats c

[TrackMe - hosts availability long term tracker]
cron_schedule = 1 * * * *
description = This scheduled report tracks and updates the data source availability KVstore based lookup
dispatch.earliest_time = -7d
dispatch.latest_time = +4h
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 900 # 15m ttl for this artefact
search  = | savedsearch "TrackMe - Data hosts abstract root tracker"\
\
`comment("#### collects latest collection state into the summary index ####")`\
| `trackme_collect_state("current_state_tracking:data_host", "data_host")`\
\
| where data_last_time_seen>relative_time(now(), "-4h-5m")\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(data_host_state, data_previous_host_state, data_host, trackme_audit_flip_temp_data_host)`\
| `trackme_outputlookup(trackme_host_monitoring, key)`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_data_host)`\
| stats c

[TrackMe - Alert on data host availability]
alert.digest_mode = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.fields = data_host
alert.suppress.period = 24h
alert.track = 1
counttype = number of events
cron_schedule = */5 * * * *
description = This alert will trigger if one or more of the data hosts are detected as unavailable or not honouring SLA policies.
dispatch.earliest_time = -5m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
disabled = true
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = trackme
request.ui_dispatch_view = search
search = | inputlookup trackme_host_monitoring\
| appendcols [ | inputlookup trackme_maintenance_mode ] | filldown maintenance_mode | where NOT maintenance_mode="enabled"\
| where data_monitored_state="enabled" AND `trackme_alerts_priority`\
| `trackme_eval_data_host_state`\
| makemv delim="," data_index\
| makemv delim="," data_sourcetype\
| makemv delim="," data_host_st_summary\
| `trackme_data_host_extract_mvstsummary` | fields - summary_first_time	summary_idx	summary_last_ingest	time, data_host_st_summary\
`comment("#### search time whitelist ####")`\
| search `trackme_get_idx_whitelist_searchtime(trackme_data_host_monitoring_whitelist_index, data_index)`\
| `apply_data_host_blacklists`\
| `trackme_data_host_group_lookup`\
| where data_host_state="red"\
| `trackme_date_format(data_last_ingest)`\
| `trackme_date_format(data_last_time_seen)`\
| `trackme_date_format(data_tracker_runtime)`\
| `trackme_date_format(data_previous_tracker_runtime)`\
| `trackme_eval_icons_host`\
| rename "* (translated)" as "*"\
| `trackme_ack_lookup(data_host, data_host)`\
| fields - monitoring, state, data_index, data_sourcetype\
| `trackme_alerts_order_data_host`

#
# Metric hosts monitoring
#

[TrackMe - metric hosts abstract root tracker]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | mstats latest(_value) as value where index=* `trackme_mstats_main_filter` `trackme_get_idx_whitelist(trackme_metric_host_monitoring_whitelist_index, metric_index)` `apply_metric_host_blacklists_data_retrieve` (host="$host$") by metric_name, index, host `trackme_mstats_span`\
| stats max(_time) as _time by metric_name, index, host\
\
`comment("#### mstats table is loaded ####")`\
\
`comment("#### apply our naming convention ####")`\
| rename host as metric_host, index as metric_index\
\
`comment("#### apply blacklist for hosts ####")`\
| `apply_metric_host_blacklists`\
\
`comment("#### extract the metric category based on the metric name naming convention, the metric_category field is the essential key of metric monitoring in trackMe concept ####")`\
| rex field=metric_name "(?<metric_category>[^.]*).{0,1}"\
\
`comment("#### apply blacklist for metric_category ####")`\
| `apply_metric_host_blacklists_metric_category`\
\
`comment("#### intermediate calculation ####")`\
| stats max(_time) as _time by metric_category, metric_index, metric_host\
\
`comment("#### calculate time delta between now and the last metric catch per metric category ####")`\
| eval metric_current_lag_sec=(now() - _time)\
\
`comment("#### lookup max lag allowed per metric ####")`\
| lookup trackme_metric_lagging_definition metric_category OUTPUT metric_max_lag_allowed\
\
`comment("#### apply default policy ####")`\
| `trackme_default_metric_host_lag`\
\
`comment("#### apply default metric state ####")`\
| `trackme_eval_metric_host_state`\
\
`comment("#### generate a metric details field ####")`\
| eval metric_details = "metric_category=" . metric_category . "|" . "metric_last_time=" . _time . "|" . "metric_max_lag_allowed=" . metric_max_lag_allowed . "|" . "metric_current_lag_sec=" . metric_current_lag_sec . "|" . "metric_host_state=" . metric_host_state . "|" . "metric_online=true"\
\
`comment("#### intermediate calculation ####")`\
| stats max(_time) as _time, values(metric_index) as metric_index, values(metric_category) as metric_category, values(metric_details) as metric_details by metric_host\
\
\
`comment("#### retrieve key, metric_details and other fields to be preserved from lookup ####")`\
| lookup trackme_metric_host_monitoring metric_host OUTPUT _key as key, metric_details as lookup_metric_details,\
metric_host_state as metric_previous_host_state, metric_tracker_runtime as metric_previous_tracker_runtime,\
metric_first_time_seen,\
metric_monitored_state, metric_monitoring_wdays, metric_override_lagging_class,\
latest_flip_state, latest_flip_time, priority\
\
`comment("#### append the current collection, then dedup, active entities will be preserve and non active will be added")`\
| inputlookup append=t trackme_metric_host_monitoring\
| eval key_collection=_key\
| eval key=coalesce(key, key_collection)\
| fields - key_collection\
\
`comment("#### handle some specials fields from the inputlookup call when hosts are not online anymore ####")`\
| eval metric_previous_host_state=if(isnull(_time), metric_host_state, metric_previous_host_state)\
| eval metric_previous_tracker_runtime=if(isnull(_time), metric_tracker_runtime, metric_previous_tracker_runtime)\
\
| search metric_host="$host$"\
| dedup metric_host\
\
`comment("#### mv fields format differ between live data and collection store data")`\
| eval metric_index=mvjoin(metric_index, ",")\
| eval metric_category=mvjoin(metric_category, ",")\
| eval metric_details=mvjoin(metric_details, ",")\
| makemv delim="," metric_index\
| makemv delim="," metric_category\
| makemv delim="," metric_details\
\
`comment("#### define time related fields ####")`\
| eval metric_last_time_seen=coalesce(_time, metric_last_time_seen)\
| eval metric_last_lag_seen=(now() - metric_last_time_seen)\
| eval metric_tracker_runtime=now()\
\
`comment("#### if the key is null, then it is a new entry in the collection ####")`\
| eval key=if(isnull(key), md5(metric_host), key)\
\
`comment("#### create a single value comma separated structure from metric_details ####")`\
| eval metric_details=mvjoin(metric_details, ",")\
\
`comment("#### merge both fields if it exists in the collection ####")`\
| eval metric_details = if(isnotnull(lookup_metric_details), metric_details . "," . lookup_metric_details, metric_details)\
| fields - lookup_metric_details\
\
`comment("#### reformat as an mvfield ####")`\
| makemv delim="," metric_details\
\
`comment("#### mvexpand and perform extractions ####")`\
| mvexpand metric_details\
| rex field=metric_details "metric_category=(?<detail_metric_category>[^\|]*)\|metric_last_time=(?<detail_metric_last_time>[^\|]*)\|metric_max_lag_allowed=(?<detail_metric_max_lag_allowed>[^\|]*)\|metric_current_lag_sec=(?<detail_metric_current_lag_sec>[^\|]*)\|metric_host_state=(?<detail_metric_host_state>[^\|]*)\|metric_online=(?<detail_metric_online>[^\|]*)"\
| rex field=metric_details "metric_category=(?<detail_metric_category>[^\|]*)\|metric_last_time=(?<detail_metric_last_time>[^\|]*)\|metric_max_lag_allowed=(?<detail_metric_max_lag_allowed>[^\|]*)\|metric_current_lag_sec=(?<detail_metric_current_lag_sec>[^\|]*)\|metric_host_state=(?<detail_metric_host_state>[^\|]*)"\
\
`comment("#### metric_details is not required anymore ####")`\
| fields - metric_details\
\
`comment("#### retrieve policies ####")`\
| lookup trackme_metric_lagging_definition metric_category as detail_metric_category OUTPUT metric_max_lag_allowed as sla_policy_detail_metric_max_lag_allowed\
\
`comment("#### conditionally define max lag allowed ####")`\
| eval detail_metric_max_lag_allowed=if(isnum(sla_policy_detail_metric_max_lag_allowed), sla_policy_detail_metric_max_lag_allowed, detail_metric_max_lag_allowed)\
\
`comment("#### define state ####")`\
| `trackme_eval_metric_category_state`\
| fields - sla_policy_detail_metric_max_lag_allowed\
\
`comment("#### runtime is equals to time ####")`\
| eval _time=metric_tracker_runtime\
\
`comment("#### perform calculations ####")`\
| eventstats latest(detail_metric_last_time) as detail_metric_last_time,  latest(detail_metric_current_lag_sec) as detail_metric_current_lag_sec, latest(detail_metric_max_lag_allowed) as detail_metric_max_lag_allowed, latest(detail_metric_host_state) as detail_metric_host_state, first(detail_metric_online) as detail_metric_online by metric_host, detail_metric_category\
\
`comment("#### conditionally define current lag ####")`\
| eval detail_metric_current_lag_sec=if(detail_metric_online="true", detail_metric_current_lag_sec, now()-detail_metric_last_time)\
\
`comment("#### define state and apply some time format ####")`\
| `trackme_eval_metric_category_state`\
| `trackme_date_format(detail_metric_last_time)`\
\
`comment("#### reformat the metric_details field and create human readable version ####")`\
| eval metric_details = "metric_category=" . detail_metric_category . "|" . "metric_last_time=" . detail_metric_last_time . "|" . "metric_max_lag_allowed=" . detail_metric_max_lag_allowed . "|" . "metric_current_lag_sec=" . detail_metric_current_lag_sec . "|" . "metric_host_state=" . detail_metric_host_state\
| eval metric_details_human  = "metric_category=" . detail_metric_category . "|" . "metric_last_time=" . 'detail_metric_last_time (translated)' . "|" . "metric_current_lag_sec=" . detail_metric_current_lag_sec . "|" . "metric_host_state=" . detail_metric_host_state\
\
`comment("#### apply blacklists ####")`\
| `apply_metric_host_blacklists`\
| `apply_metric_host_blacklists_detail_metric_category`\
\
`comment("#### format a table ####")`\
| stats first(key) as key, min(metric_first_time_seen) as metric_first_time_seen, max(metric_last_time_seen) as metric_last_time_seen, values(metric_index) as metric_index,\
values(metric_category) as metric_category, values(metric_details) as metric_details, values(metric_details_human) as metric_details_human,\
latest(metric_last_lag_seen) as metric_last_lag_seen, max(metric_tracker_runtime) as metric_tracker_runtime, first(metric_monitoring_wdays) as metric_monitoring_wdays,\
first(metric_monitored_state) as metric_monitored_state, first(priority) as priority, first(metric_previous_host_state) as metric_previous_host_state, first(metric_previous_tracker_runtime) as metric_previous_tracker_runtime,\
first(latest_flip_time) as latest_flip_time, first(latest_flip_state) as latest_flip_state,\
first(metric_override_lagging_class) as metric_override_lagging_class by metric_host\
\
`comment("#### handle first time seen ####")`\
| eval metric_first_time_seen=if(isnull(metric_first_time_seen), metric_last_time_seen, metric_first_time_seen)\
\
`comment("#### define state and priority ####")`\
| `trackme_eval_metric_host_state`\
| `trackme_default_priority`\
\
`comment("handle logical group mapping")`\
| `trackme_metric_host_group_lookup`\
\
`comment("#### search time whitelist ####")`\
| search `trackme_get_idx_whitelist_searchtime(trackme_metric_host_monitoring_whitelist_index, metric_index)`\
\
`comment("#### merge ####")`\
| stats first(key) as key, first(latest_flip_time) as latest_flip_time, first(latest_flip_state) as latest_flip_state, first(current_latest_flip_time) as current_latest_flip_time, first(current_latest_flip_state) as current_latest_flip_state, values(*) as "*" by metric_host\
\
`comment("#### some final steps, define the state if new, by safety requires a metric_category, define the monitored state ####")`\
| eval metric_previous_host_state=if(isnull(metric_previous_host_state), "discovered", metric_previous_host_state)\
| eval metric_previous_tracker_runtime=if(isnull(metric_previous_tracker_runtime), now(), metric_previous_tracker_runtime)\
| eval latest_flip_state=if(isnull(latest_flip_state), metric_previous_host_state, latest_flip_state)\
| eval latest_flip_time=if(isnull(latest_flip_time), metric_previous_tracker_runtime, latest_flip_time)\
| where isnotnull(metric_category)\
| `trackme_default_metric_host_monitored_state`

[TrackMe - metric hosts collection based table report]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | inputlookup trackme_metric_host_monitoring\
| eval keyid=_key\
| `apply_metric_host_blacklists`\
| eval metric_index_raw=metric_index, metric_category_raw=metric_category, metric_details_raw=metric_details\
| makemv delim="," metric_index\
| makemv delim="," metric_category\
| makemv delim="," metric_details\
| eval index=metric_index | search `trackme_get_idx_whitelist(trackme_metric_host_monitoring_whitelist_index, metric_index)` | fields - index\
| mvexpand metric_details\
| rex field=metric_details "metric_category=(?<detail_metric_category>[^\|]*)\|metric_last_time=(?<detail_metric_last_time>[^\|]*)\|metric_max_lag_allowed=(?<detail_metric_max_lag_allowed>[^\|]*)\|metric_current_lag_sec=(?<detail_metric_current_lag_sec>[^\|]*)\|metric_host_state=(?<detail_metric_host_state>[^\|]*)"\
| `trackme_date_format(detail_metric_last_time)`\
| eval metric_details_human  = "metric_category=" . detail_metric_category . "|" . "metric_last_time=" . 'detail_metric_last_time (translated)' . "|" . "metric_current_lag_sec=" . detail_metric_current_lag_sec . "|" . "metric_host_state=" . detail_metric_host_state\
| fields - detail_* | where NOT match(metric_details, "metric_category=trackme") | stats values(*) as "*" by keyid

[TrackMe - metric hosts availability tracker]
cron_schedule = */5 * * * *
description = This scheduled report tracks and updates the metric hosts availability KVstore based lookup
dispatch.earliest_time = -5m
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 300 # 5m ttl for this artefact
search = | savedsearch "TrackMe - metric hosts abstract root tracker" host=*\
\
`comment("#### collects latest collection state into the summary index ####")`\
| `trackme_collect_state("current_state_tracking:metric_host", "metric_host")`\
\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(metric_host_state, metric_previous_host_state, metric_host, trackme_audit_flip_temp_metric_host)`\
\
| eval metric_category=mvjoin(metric_category, ","), metric_index=mvjoin(metric_index, ","), metric_details=mvjoin(metric_details, ",")\
| search NOT [ | inputlookup trackme_audit_changes | where action="success" AND change_type="delete permanent" AND object_category="metric_host" | eval _time=time/1000 | where _time>relative_time(now(), "-7d") | table object | dedup object | sort limit=0 object | rename object as metric_host ]\
| eval metric_monitored_state=if(metric_last_time_seen<=`trackme_auto_disablement_period`, "disabled", metric_monitored_state)\
| `trackme_outputlookup(trackme_metric_host_monitoring, key)`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_metric_host)`\
| stats c

[trackme_update_metric_host_target_by_metric_host]
dispatch.earliest_time = -5m
dispatch.latest_time = now
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | savedsearch "TrackMe - metric hosts abstract root tracker" host="$host$"\
\
`comment("#### collects latest collection state into the summary index ####")`\
| `trackme_collect_state("current_state_tracking:metric_host", "metric_host")`\
\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(metric_host_state, metric_previous_host_state, metric_host, trackme_audit_flip_temp_metric_host)`\
\
| eval metric_category=mvjoin(metric_category, ","), metric_index=mvjoin(metric_index, ","), metric_details=mvjoin(metric_details, ",")\
| search NOT [ | inputlookup trackme_audit_changes where (object_type="metric_host" AND object="$host$") | where action="success" AND change_type="delete permanent" AND object_category="metric_host" | eval _time=time/1000 | where _time>relative_time(now(), "-7d") | table object | dedup object | sort limit=0 object | rename object as metric_host ]\
| eval metric_monitored_state=if(metric_last_time_seen<=`trackme_auto_disablement_period`, "disabled", metric_monitored_state)\
| `trackme_outputlookup(trackme_metric_host_monitoring, key)`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_metric_host)`\
| stats c

[trackMe - metric host table report]
dispatch.earliest_time = -5m
dispatch.latest_time = now
is_visible = false
search = | savedsearch "TrackMe - metric hosts abstract root tracker" host=*\
| eval metric_host_state=if(match(metric_details, "metric_host_state=red"), "red", "green")\
| `trackme_metric_host_group_lookup`\
| `trackme_date_format(metric_last_time_seen)` | rename "metric_last_time_seen (translated)" as "last time"

[trackMe - metric host live report]
dispatch.earliest_time = -5m
dispatch.latest_time = now
is_visible = false
search = | mstats latest(_value) as value where index=* metric_name="*" host="$host$" by metric_name, index, host span=1s\
| stats max(_time) as _time by metric_name, index, host\
| rex field=metric_name "(?<metric_category>[^\.]*)\.{0,1}"\
| stats values(metric_name) as metric_name, max(_time) as _time by metric_category, index, host\
| eval metric_current_lag_sec=(now() - _time)\
| lookup trackme_metric_lagging_definition metric_category OUTPUT metric_max_lag_allowed\
| eval metric_max_lag_allowed=if(isnull(metric_max_lag_allowed), "300", metric_max_lag_allowed)\
| eval metric_host_state=if(match(metric_details, "metric_host_state=red"), "red", "green")\
| stats max(_time) as "metric_last_time_seen", first(metric_max_lag_allowed) as metric_max_lag_allowed, values(index) as index, values(metric_name) as metric_name, first(metric_host_state) as metric_host_state by metric_category\
| `trackme_date_format(metric_last_time_seen)` | fields - metric_last_time_seen | rename "metric_last_time_seen (translated)" as metric_last_time_seen\
| fields index, metric_category, metric_name, metric_last_time_seen, metric_host_state

[trackMe - metric per host table report]
dispatch.earliest_time = -5m
dispatch.latest_time = now
is_visible = false
search = | savedsearch "TrackMe - metric hosts abstract root tracker" host="$host$"\
| fields metric_details_human\
| mvexpand metric_details_human\
| rex field=metric_details_human "metric_category=(?<metric_category>[^\|]*)\|metric_last_time=(?<metric_last_time>[^\|]*)\|metric_current_lag_sec=(?<metric_current_lag_sec>[^\|]*)\|metric_host_state=(?<metric_host_state>\w*)"\
| lookup trackme_metric_lagging_definition metric_category OUTPUT metric_max_lag_allowed\
| `trackme_default_metric_host_lag`\
| `trackme_eval_icons_metric_host_state_only`\
| eval duration=tostring(metric_current_lag_sec, "duration")\
| fields metric_category metric_last_time metric_max_lag_allowed metric_current_lag_sec duration state metric_host_state

[TrackMe - Alert on metric host availability]
alert.digest_mode = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.fields = metric_host
alert.suppress.period = 24h
alert.track = 1
counttype = number of events
cron_schedule = */5 * * * *
description = This alert will trigger if one or more of the metric hosts are detected in red mode, which one or more monitored metric unavailable of not honouring SLA policies.
dispatch.earliest_time = -5m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
disabled = true
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = trackme
request.ui_dispatch_view = search
search = | inputlookup trackme_metric_host_monitoring\
| makemv delim="," metric_index\
`comment("#### search time whitelist ####")`\
| search `trackme_get_idx_whitelist_searchtime(trackme_metric_host_monitoring_whitelist_index, metric_index)`\
| `apply_metric_host_blacklists`\
| appendcols [ | inputlookup trackme_maintenance_mode ] | filldown maintenance_mode | where NOT maintenance_mode="enabled"\
| where metric_monitored_state="enabled" AND `trackme_alerts_priority`\
| where metric_host_state="red"\
| `trackme_date_format(metric_last_time_seen)`\
| eval "last time"='metric_last_time_seen (translated)'\
| `trackme_date_format(metric_tracker_runtime)`\
| `trackme_date_format(metric_previous_tracker_runtime)`\
| `trackme_eval_icons_metric_host`\
| rename "* (translated)" as "*"\
| makemv metric_category delim=","\
| makemv metric_details delim=","\
| `trackme_ack_lookup(metric_host, metric_host)`\
| `trackme_alerts_order_metric_host`

#
# Summary investigator
#

# These trackers perform updates for outliers detection based on high performance mstats searches

[TrackMe - Volume -24h Outliers update tracker]
cron_schedule = 1 1 * * *
description = This scheduled report tracks and updates the summary investigator lookup which is used for behaviour analytic purposes
dispatch.earliest_time = -24h
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
schedule_window = 15
dispatch.ttl = 900 # 15m ttl for this artefact
search  = | `trackme_summary_investigator_mstats(-24h)`\
\
`comment("#### Call the trackme_summary_investigator_define_bound_abstract macro ####")`\
`trackme_summary_investigator_define_bound_abstract`\
\
`comment("#### store and update the collection entities ####")`\
| eval key = md5(object_category . ":" . object)\
| `trackme_outputlookup(trackme_summary_investigator_volume_outliers, key)`\
| stats c

[TrackMe - Volume -48h Outliers update tracker]
cron_schedule = 1 2 * * *
description = This scheduled report tracks and updates the summary investigator lookup which is used for behaviour analytic purposes
dispatch.earliest_time = -48h
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
schedule_window = 15
dispatch.ttl = 900 # 15m ttl for this artefact
search  = | `trackme_summary_investigator_mstats(-48h)`\
\
`comment("#### Call the trackme_summary_investigator_define_bound_abstract macro ####")`\
`trackme_summary_investigator_define_bound_abstract`\
\
`comment("#### store and update the collection entities ####")`\
| eval key = md5(object_category . ":" . object)\
| `trackme_outputlookup(trackme_summary_investigator_volume_outliers, key)`\
| stats c

[TrackMe - Volume -7d Outliers update tracker]
cron_schedule = 1 3 * * *
description = This scheduled report tracks and updates the summary investigator lookup which is used for behaviour analytic purposes
dispatch.earliest_time = -7d
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
schedule_window = 15
dispatch.ttl = 900 # 15m ttl for this artefact
search  = | `trackme_summary_investigator_mstats(-7d)`\
\
`comment("#### Call the trackme_summary_investigator_define_bound_abstract macro ####")`\
`trackme_summary_investigator_define_bound_abstract`\
\
`comment("#### store and update the collection entities ####")`\
| eval key = md5(object_category . ":" . object)\
| `trackme_outputlookup(trackme_summary_investigator_volume_outliers, key)`\
| stats c

[TrackMe - Volume -30d Outliers update tracker]
cron_schedule = 1 4 * * *
description = This scheduled report tracks and updates the summary investigator lookup which is used for behaviour analytic purposes
dispatch.earliest_time = -30d
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
schedule_window = 15
dispatch.ttl = 900 # 15m ttl for this artefact
search  = | `trackme_summary_investigator_mstats(-30d)`\
\
`comment("#### Call the trackme_summary_investigator_define_bound_abstract macro ####")`\
`trackme_summary_investigator_define_bound_abstract`\
\
`comment("#### store and update the collection entities ####")`\
| eval key = md5(object_category . ":" . object)\
| `trackme_outputlookup(trackme_summary_investigator_volume_outliers, key)`\
| stats c

[TrackMe - Data source entity refresh]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search  = | `trackme_tstats` max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount where `trackme_tstats_main_filter` `trackme_get_idx_whitelist(trackme_data_source_monitoring_whitelist_index, data_index)` `apply_data_source_blacklists_data_retrieve` index="$index$" sourcetype="$sourcetype$" by index, sourcetype, host\
\
`comment("#### tstats result table is loaded ####")`\
| eval data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\
\
`comment("#### intermediate calculation ####")`\
| stats max(data_last_ingest) as data_last_ingest, min(data_first_time_seen) as data_first_time_seen, max(data_last_time_seen) as data_last_time_seen, avg(data_last_ingestion_lag_seen) as data_last_ingestion_lag_seen, sum(data_eventcount) as data_eventcount by index, sourcetype\
| eval data_last_ingestion_lag_seen=round(data_last_ingestion_lag_seen, 0)\
\
`comment("#### rename index and sourcetype ####")`\
| rename index as data_index, sourcetype as data_sourcetype\
\
`comment("#### create the data_name value ####")`\
| eval data_name=data_index . ":" . data_sourcetype\
\
`comment("#### call the abstract macro ####")`\
`trackme_data_source_tracker_abstract`\
\
| where data_name="$data_name$"\
\
`comment("#### output flipping change status if changes ####")`\
| `trackme_get_flip(data_source_state, data_previous_source_state, data_name, trackme_audit_flip_temp_manual_refresh)`\
\
`comment("#### run collect and updates the KVstore ####")`\
| `trackme_outputlookup(trackme_data_source_monitoring, key)`\
| `trackme_mcollect(data_name, data_source, "metric_name:trackme.eventcount_4h=data_eventcount, metric_name:trackme.hostcount_4h=dcount_host, metric_name:trackme.lag_event_sec=data_last_lag_seen, metric_name:trackme.lag_ingestion_sec=data_last_ingestion_lag_seen", "object_category, object, OutlierTimePeriod, enable_behaviour_analytic")`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_manual_refresh)`\
| stats c

[TrackMe - Data host entity refresh]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search  = | `trackme_tstats` max(_indextime) as data_last_ingest, min(_time) as data_first_time_seen, max(_time) as data_last_time_seen, count as data_eventcount where index=* `trackme_tstats_main_filter` `trackme_get_idx_whitelist(trackme_data_host_monitoring_whitelist_index, data_index)` `apply_data_host_blacklists_data_retrieve` host="$host$" by index, sourcetype, host\
| eval data_last_ingestion_lag_seen=data_last_ingest-data_last_time_seen\
| `trackme_data_host_tracker_abstract`\
| where data_host="$host$"\
| `trackme_get_flip(data_host_state, data_previous_host_state, data_host, trackme_audit_flip_temp_data_host)`\
| `trackme_outputlookup(trackme_host_monitoring, key)`\
| `trackme_mcollect(data_host, data_host, "metric_name:trackme.eventcount_4h=data_eventcount, metric_name:trackme.lag_event_sec=data_last_lag_seen, metric_name:trackme.lag_ingestion_sec=data_last_ingestion_lag_seen", "object_category, object, OutlierTimePeriod, enable_behaviour_analytic")`\
| stats c\
| `trackme_collect_flip(trackme_audit_flip_temp_data_host)`\
| stats c

# Various

[TrackMe - Audit changes night purge]
cron_schedule = 30 3 * * *
description = This scheduled report purges old entries from the audit changes collection
dispatch.earliest_time = -5m
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
search  = | inputlookup trackme_audit_changes | sort limit=0 - time | eval _time=time/1000\
| `trackme_audit_changes_retention`\
| outputlookup trackme_audit_changes\
| stats count

# Ack trackers

[TrackMe - Ack tracker]
cron_schedule = 1-56/5 * * * *
description = This scheduled report tracks and updates the Ack KVstore based lookup
dispatch.earliest_time = -5m
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 300 # 5m ttl for this artefact
search = | inputlookup trackme_alerts_ack | eval keyid=_key\
| eval limit_expiration=ack_expiration-300\
| eval ack_state=if(now()>=limit_expiration, "inactive", ack_state)\
| lookup trackme_data_source_monitoring data_name as object OUTPUTNEW data_source_state as object_current_state\
| lookup trackme_host_monitoring data_host as object OUTPUTNEW data_host_state as object_current_state\
| lookup trackme_metric_host_monitoring metric_host as object OUTPUTNEW metric_host_state as object_current_state\
| eval ack_state=if(object_current_state="green", "inactive", ack_state)\
| eval ack_expiration=if(ack_state="inactive", "N/A", ack_expiration), ack_mtime=if(ack_state="inactive", "N/A", ack_mtime) | fields keyid, *\
| where ack_state="active"\
| outputlookup trackme_alerts_ack\
| stats c

# TrackMe audit change notification tracker
# The purpose of this alert is to provide a notification flow for online team working, such as sharing in a Slack channel when modifications are
# performed by administrators.

[TrackMe - Audit change notification tracker]
alert.digest_mode = 0
alert.severity = 1
alert.suppress = 1
alert.suppress.fields = _time, user, change_type, object_category, object
alert.suppress.period = 24h
alert.track = 1
counttype = number of events
cron_schedule = */5 * * * *
description = This alert can be activated and used to share with other administrators any change that is operated within the application
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
disabled = true
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
dispatch.ttl = 300 # 5m ttl for this artefact
search = | inputlookup trackme_audit_changes\
| sort limit=0 - time\
| eval _time=time/1000 | eval time=strftime(_time, "%c") | addinfo | where _time>=info_min_time AND (_time<=info_max_time OR info_max_time="+Infinity")\
| table _time, time, user, action, change_type, object_category, object, object_attrs, comment | fillnull value="N/A"

# trackme get table

[trackme_get_data_source_table_by_key]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | inputlookup trackme_data_source_monitoring where _key=$key$\
| eval keyid=_key\
| `trackme_eval_data_source_state`\
| `trackme_default_priority`\
| `trackme_date_format(data_last_time_seen)`\
| `trackme_date_format(data_last_time_seen_idx)`\
| `trackme_date_format(data_last_ingest)`\
| `trackme_eval_icons`\
| fillnull value="red" data_source_state\
| fillnull data_last_lag_seen, data_last_ingestion_lag_seen\
| eval "lag summary (lag event / lag ingestion)" = if(data_last_lag_seen>60, tostring(data_last_lag_seen, "duration"), data_last_lag_seen . " sec") . " / " . if(data_last_ingestion_lag_seen>60, tostring(data_last_ingestion_lag_seen, "duration"), data_last_ingestion_lag_seen . " sec")\
| rename "data_last_time_seen (translated)" as "last time", "data_last_ingest (translated)" as "last ingest", "data_last_time_seen_idx (translated)" as "last time idx"\
| lookup local=t trackme_data_source_monitoring data_name OUTPUT data_source_state as data_previous_source_state, data_tracker_runtime as data_previous_tracker_runtime, latest_flip_state, latest_flip_time\
| `trackme_date_format("latest_flip_time")`\
| fillnull value="N/A" latest_flip_state, latest_flip_time, "latest_flip_time (translated)"\
| `trackme_lookup_data_sampling`

[trackme_get_data_host_table_by_key]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | inputlookup trackme_host_monitoring\
| eval keyid=_key\
| eval data_index_raw=data_index, data_sourcetype_raw=data_sourcetype\
| makemv delim="," data_index\
| makemv delim="," data_sourcetype\
| `trackme_eval_data_host_state`\
| `trackme_default_priority`\
| `trackme_data_host_group_lookup`\
| `trackme_date_format(data_last_time_seen)`\
| `trackme_date_format(data_last_time_seen_idx)`\
| `trackme_date_format(data_last_ingest)`\
| `trackme_eval_icons_host`\
| where keyid="$key$"\
| rename "data_last_ingest (translated)" as "last ingest", "data_last_time_seen (translated)" as "last time"\
| lookup local=t trackme_host_monitoring data_host OUTPUT data_host_state as data_previous_host_state, data_tracker_runtime as data_previous_tracker_runtime, latest_flip_state, latest_flip_time\
| `trackme_date_format("latest_flip_time")`\
| fillnull value="N/A" latest_flip_state, latest_flip_time, "latest_flip_time (translated)"\
| fillnull data_last_lag_seen, data_last_ingestion_lag_seen | eval "lag summary (lag event / lag ingestion)" = if(data_last_lag_seen>60, tostring(data_last_lag_seen, "duration"), data_last_lag_seen . " sec") . " / " . if(data_last_ingestion_lag_seen>60, tostring(data_last_ingestion_lag_seen, "duration"), data_last_ingestion_lag_seen . " sec")

[trackme_get_metric_host_table_by_key]
request.ui_dispatch_app = trackme
request.ui_dispatch_view = trackme
is_visible = false
search = | savedsearch "TrackMe - metric hosts collection based table report"\
| `trackme_date_format("metric_last_time_seen")`\
| rename "metric_last_time_seen (translated)" as "last time"\
| `trackme_eval_metric_host_state`\
| `trackme_metric_host_group_lookup`\
| `trackme_eval_icons_metric_host`\
| `trackme_date_format("latest_flip_time")`\
| `trackme_eval_icons_metric_host`\
| where keyid="$key$"\
| fillnull value="N/A" latest_flip_state, latest_flip_time, "latest_flip_time (translated)"

[Verify trackMe alerting maintenance status]
cron_schedule = */5 * * * *
description = This scheduled report verifies the default maintenance status, if no status is set, it will be disabled by default
dispatch.earliest_time = -5m
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = search
schedule_window = 5
run_on_startup = true
dispatch.ttl = 300 # 5m ttl for this artefact
search  = | makeresults\
| appendcols [ | inputlookup trackme_maintenance_mode ]\
| eval maintenance_mode=if(isnull(maintenance_mode), "disabled", maintenance_mode), time_updated=if(isnull(time_updated), now(), time_updated)\
| eval maintenance_mode=if(isnotnull(maintenance_mode_end) AND maintenance_mode_end<now(), "disabled", maintenance_mode)\
| eval maintenance_mode=if(isnotnull(maintenance_mode_start) AND isnotnull(maintenance_mode_end) AND now()>=maintenance_mode_start AND maintenance_mode_end>now(), "enabled", maintenance_mode)\
| eval maintenance_mode_start=if(isnotnull(maintenance_mode_end) AND maintenance_mode_end<now(), "", maintenance_mode_start), maintenance_mode_end=if(isnotnull(maintenance_mode_end) AND maintenance_mode_end<now(), "", maintenance_mode_end)\
| eval time_updated=if(maintenance_mode="enabled" AND isnotnull(maintenance_mode_end) AND maintenance_mode_end<now(), now, time_updated)\
| eval time_updated=if(maintenance_mode="enabled" AND isnotnull(maintenance_mode_start) AND isnotnull(maintenance_mode_end) AND now()>=maintenance_mode_start AND maintenance_mode_end<now(), now, time_updated)\
| fields - time | table maintenance_mode, maintenance_mode_start, maintenance_mode_end, time_updated | outputlookup trackme_maintenance_mode

# Wrapper for elastic searches
[runSPL]
description = Runs the SPL that's passed in
is_visible = false
search = $spl$

[TrackMe - Backup KVstore collections and purge older backup files]
alert.track = 0
cron_schedule = 0 2 * * *
description = This scheduled report performs a backup of the TrackMe KVstore collections, and purges older backup archive files.
dispatch.earliest_time = -5m
dispatch.latest_time = now
enableSched = 1
request.ui_dispatch_app = trackme
request.ui_dispatch_view = search
schedule_window = 15
search = | trackme url=/services/trackme/v1/backup_and_restore/backup mode=post\
| append [ | trackme url=/services/trackme/v1/backup_and_restore/backup mode=delete body="{'retention_days': '7'}" ]
